{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "trial Tri 1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thangvle/smart_pros/blob/master/trial_Tri_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "D_9jqctGrEXQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4111
        },
        "outputId": "1add9a38-76ff-4b1b-d101-e60fb831aa1f"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import pylab as pl\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "# need the following 2 libraries to read csv files\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "# importing EMG data\n",
        "  # first download the csv file to tensorflow & label it\n",
        "!wget -O sheetnamedude.csv https://raw.githubusercontent.com/thangvle/smart_pros/master/EMG_label_data_muscle1.csv\n",
        "  # now we can use that name to read the data\n",
        "data = pd.read_csv(\"sheetnamedude.csv\")\n",
        "\n",
        "# turn time and voltage to 1D array\n",
        "data.columns = [\"time_stamp\", \"voltage\", \"time\", \"label\"]\n",
        "time = data[\"time\"].values\n",
        "voltage = data[\"voltage\"].values\n",
        "labels = data[\"label\"].values\n",
        "print(\"Time\")\n",
        "print(time)\n",
        "print(\"voltage\")\n",
        "print(voltage)\n",
        "print(\"label\")\n",
        "print(labels)\n",
        "\n",
        "train_data, test_data = train_test_split(data, train_size=0.8)\n",
        "print(\"train_data\")\n",
        "print(train_data)\n",
        "print(\"test_data\")\n",
        "print(test_data)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "TODO\n",
        "\n",
        "- Initialize variable for time and voltage as 1D array\n",
        "- Initialize Weight function\n",
        "- Create loss function and optimizer (SGD)\n",
        "- tf.session\n",
        "\"\"\"\n",
        "\n",
        "def conv_relu(inputs, filters, k_size, stride, padding, scope_name):\n",
        "\n",
        "    # method that does convolution + relu on inputs\n",
        "\n",
        "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
        "        in_channels = inputs.shape[-1]\n",
        "        kernel = tf.get_variable('kernel',\n",
        "                                [k_size, k_size, in_channels, filters],\n",
        "                                initializer=tf.truncated_normal_initializer())\n",
        "        biases = tf.get_variable('biases',\n",
        "                                [filters],\n",
        "                                initializer=tf.random_normal_initializer())\n",
        "        conv = tf.nn.conv1d(inputs, kernel, strides=[1, stride, stride, 1], padding=padding)\n",
        "    return tf.nn.relu(conv + biases, name=scope.name)\n",
        "\n",
        "def maxpool(inputs, ksize, stride, padding='VALID', scope_name='pool'):\n",
        "    '''A method that does max pooling on inputs'''\n",
        "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
        "        pool = tf.nn.max_pool(inputs,\n",
        "                            ksize=[1, ksize, ksize, 1],\n",
        "                            strides=[1, stride, stride, 1],\n",
        "                            padding=padding)\n",
        "    return pool\n",
        "\n",
        "def fully_connected(inputs, out_dim, scope_name='fc'):\n",
        "    # A fully connected linear layer on inputs\n",
        "\n",
        "    with tf.variable_scope(scope_name, reuse=tf.AUTO_REUSE) as scope:\n",
        "        in_dim = inputs.shape[-1]\n",
        "        w = tf.get_variable('weights', [in_dim, out_dim],\n",
        "                            initializer=tf.truncated_normal_initializer())\n",
        "        b = tf.get_variable('biases', [out_dim],\n",
        "                            initializer=tf.constant_initializer(0.0))\n",
        "        out = tf.matmul(inputs, w) + b\n",
        "    return out\n",
        "\n",
        "\n",
        "class ConvNet(object):\n",
        "  def __init__(self):\n",
        "\t     self.learing_rate = 0.001\t\t# set up learning rate\n",
        "  self.batch_size = 300\t\t\t# split up data in 1 epoch\n",
        "self.keep_prob = tf.constant(0.75)\t# probablity\n",
        "self.gstep = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
        "self.n_classes = 10\n",
        "self.skip_step = 20\n",
        "self.n_test = 10000\n",
        "self.training = False\n",
        "def get_data(self):\n",
        "\t       with tf.name_scope('data'):\n",
        "               # need to have label with the train_data and test_data\n",
        "       \t    # create dataset\n",
        "       \t\t   train_data = tf.data.Dataset.from_tensor_slices(train)\n",
        "train_data = train_data.shuffle(10000)\n",
        "train_data = train_data.batch(batch_size)\n",
        "\n",
        "test_data = tf.data.Dataset.from_tensor_slices(test)\n",
        "test_data = test_data.batch(batch_size)\n",
        "\n",
        "# creating iterator\n",
        "iterator = tf.data.Iterator.from_structure(train_data.output_typestrain_data.output_shapes)\n",
        "# reshape array here if needed\n",
        "\n",
        "# Initializer for train and test Dataset\n",
        "self.train_init = iterator.make_initializer(train_data)\n",
        "self.test_init = iterator.make_initializer(test_data)\n",
        "\n",
        "def inference(self):\n",
        "        conv1 = conv_relu(inputs=self.img,\n",
        "                        filters=32,\n",
        "                        k_size=5,\n",
        "                        stride=1,\n",
        "                        padding='SAME',\n",
        "                        scope_name='conv1')\n",
        "        pool1 = maxpool(conv1, 2, 2, 'VALID', 'pool1')\n",
        "\n",
        "        fc = fully_connected(pool1, 1024, 'fc')\n",
        "        dropout = tf.nn.dropout(tf.nn.relu(fc), self.keep_prob, name='relu_dropout')\n",
        "        self.logits = fully_connected(dropout, self.n_classes, 'logits')\n",
        "\n",
        "def loss(self):\n",
        "        '''\n",
        "        define loss function\n",
        "        use softmax cross entropy with logits as the loss function\n",
        "        compute mean cross entropy, softmax is applied internally\n",
        "        '''\n",
        "        #\n",
        "        with tf.name_scope('loss'):\n",
        "            entropy = tf.nn.softmax_cross_entropy_with_logits(labels=self.label, logits=self.logits)\n",
        "            self.loss = tf.reduce_mean(entropy, name='loss')\n",
        "\n",
        "def optimize(self):\n",
        "    '''\n",
        "    Define training op\n",
        "    using Adam Gradient Descent to minimize cost\n",
        "    '''\n",
        "    self.opt = tf.train.AdamOptimizer(self.lr).minimize(self.loss,\n",
        "                                            global_step=self.gstep)\n",
        "\n",
        "def summary(self):\n",
        "    '''\n",
        "    Create summaries to write on TensorBoard\n",
        "    '''\n",
        "    with tf.name_scope('summaries'):\n",
        "        tf.summary.scalar('loss', self.loss)\n",
        "        tf.summary.scalar('accuracy', self.accuracy)\n",
        "        tf.summary.histogram('histogram loss', self.loss)\n",
        "        self.summary_op = tf.summary.merge_all()\n",
        "\n",
        "def eval(self):\n",
        "    '''\n",
        "    Count the number of right predictions in a batch\n",
        "    '''\n",
        "    with tf.name_scope('predict'):\n",
        "        preds = tf.nn.softmax(self.logits)\n",
        "        correct_preds = tf.equal(tf.argmax(preds, 1), tf.argmax(self.label, 1))\n",
        "        self.accuracy = tf.reduce_sum(tf.cast(correct_preds, tf.float32))\n",
        "\n",
        "def build(self):\n",
        "    '''\n",
        "    Build the computation graph\n",
        "    '''\n",
        "    self.get_data()\n",
        "    self.inference()\n",
        "    self.loss()\n",
        "    self.optimize()\n",
        "    self.eval()\n",
        "    self.summary()\n",
        "\n",
        "def train_one_epoch(self, sess, saver, init, writer, epoch, step):\n",
        "    start_time = time.time()\n",
        "    sess.run(init)\n",
        "    self.training = True\n",
        "    total_loss = 0\n",
        "    n_batches = 0\n",
        "    try:\n",
        "        while True:\n",
        "            _, l, summaries = sess.run([self.opt, self.loss, self.summary_op])\n",
        "            writer.add_summary(summaries, global_step=step)\n",
        "            if (step + 1) % self.skip_step == 0:\n",
        "                print('Loss at step {0}: {1}'.format(step, l))\n",
        "            step += 1\n",
        "            total_loss += l\n",
        "            n_batches += 1\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "    saver.save(sess, 'checkpoints/convnet_mnist/mnist-convnet', step)\n",
        "    print('Average loss at epoch {0}: {1}'.format(epoch, total_loss/n_batches))\n",
        "    print('Took: {0} seconds'.format(time.time() - start_time))\n",
        "    return step\n",
        "\n",
        "def eval_once(self, sess, init, writer, epoch, step):\n",
        "    start_time = time.time()\n",
        "    sess.run(init)\n",
        "    self.training = False\n",
        "    total_correct_preds = 0\n",
        "    try:\n",
        "        while True:\n",
        "            accuracy_batch, summaries = sess.run([self.accuracy, self.summary_op])\n",
        "            writer.add_summary(summaries, global_step=step)\n",
        "            total_correct_preds += accuracy_batch\n",
        "    except tf.errors.OutOfRangeError:\n",
        "        pass\n",
        "\n",
        "    print('Accuracy at epoch {0}: {1} '.format(epoch, total_correct_preds/self.n_test))\n",
        "    print('Took: {0} seconds'.format(time.time() - start_time))\n",
        "\n",
        "def train(self, n_epochs):\n",
        "    '''\n",
        "    The train function alternates between training one epoch and evaluating\n",
        "    '''\n",
        "    utils.safe_mkdir('checkpoints')\n",
        "    utils.safe_mkdir('checkpoints/convnet_mnist')\n",
        "    writer = tf.summary.FileWriter('./graphs/convnet', tf.get_default_graph())\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        sess.run(tf.global_variables_initializer())\n",
        "        saver = tf.train.Saver()\n",
        "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/convnet_mnist/checkpoint'))\n",
        "        if ckpt and ckpt.model_checkpoint_path:\n",
        "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
        "\n",
        "        step = self.gstep.eval()\n",
        "\n",
        "        for epoch in range(n_epochs):\n",
        "            step = self.train_one_epoch(sess, saver, self.train_init, writer, epoch, step)\n",
        "            self.eval_once(sess, self.test_init, writer, epoch, step)\n",
        "    writer.close()\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    model = ConvNet()\n",
        "    model.build()\n",
        "    model.train(n_epochs=30)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2019-03-24 01:28:05--  https://raw.githubusercontent.com/thangvle/smart_pros/master/EMG_label_data_muscle1.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 151.101.0.133, 151.101.64.133, 151.101.128.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|151.101.0.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11285 (11K) [text/plain]\n",
            "Saving to: ‘sheetnamedude.csv’\n",
            "\n",
            "\rsheetnamedude.csv     0%[                    ]       0  --.-KB/s               \rsheetnamedude.csv   100%[===================>]  11.02K  --.-KB/s    in 0s      \n",
            "\n",
            "2019-03-24 01:28:05 (107 MB/s) - ‘sheetnamedude.csv’ saved [11285/11285]\n",
            "\n",
            "Time\n",
            "[3010 3020 3032 3042 3052 3063 3074 3085 3095 3105 3117 3127 3137 3148\n",
            " 3159 3170 3180 3190 3202 3212 3222 3233 3244 3254 3265 3275 3287 3297\n",
            " 3307 3318 3329 3339 3350 3360 3371 3382 3392 3403 3414 3424 3435 3445\n",
            " 3456 3467 3477 3488 3499 3509 3520 3530 3540 3552 3562 3573 3584 3594\n",
            " 3605 3615 3625 3637 3647 3658 3668 3679 3690 3700 3710 3722 3732 3743\n",
            " 3753 3764 3775 3785 3795 3807 3817 3828 3838 3849 3860 3870 3880 3892\n",
            " 3902 3913 3923 3934 3945 3955 3965 3977 3987 3998 4008 4019 4030 4040\n",
            " 4050 4062 4072 4083 4093 4104 4115 4125 4135 4147 4157 4167 4178 4189\n",
            " 4200 4210 4220 4232 4242 4252 4263 4274 4285 4295 4305 4317 4327 4337\n",
            " 4348 4359 4370 4380 4390 4402 4412 4422 4433 4444 4454 4465 4475 4487\n",
            " 4497 4507 4518 4529 4539 4550 4560 4572 4582 4592 4603 4614 4624 4635\n",
            " 4645 4656 4667 4677 4688 4699 4709 4720 4730 4741 4752 4762 4773 4784\n",
            " 4794 4805 4815 4826 4837 4847 4857 4869 4879 4890 4900 4911 4922 4932\n",
            " 4942 4954 4964 4974 4985 4996 5007 5017 5027 5039 5049 5059 5070 5081\n",
            " 5092 5102 5112 5124 5134 5144 5155 5166 5176 5187 5197 5209 5219 5229\n",
            " 5240 5251 5261 5272 5282 5293 5304 5314 5325 5336 5346 5357 5367 5378\n",
            " 5389 5399 5409 5421 5431 5442 5452 5463 5474 5484 5494 5506 5516 5526\n",
            " 5537 5548 5559 5569 5579 5591 5601 5611 5622 5633 5643 5654 5664 5675\n",
            " 5686 5696 5707 5718 5728 5739 5749 5760 5771 5781 5791 5803 5813 5824\n",
            " 5834 5844 5856 5866 5876 5888 5898 5908 5919 5929 5941 5951 5961 5972\n",
            " 5983 5993 6004 6014 6025 6036 6046 6056 6068 6078 6089 6099 6110 6121\n",
            " 6131 6141 6153 6163 6174]\n",
            "voltage\n",
            "[0.68 0.66 0.64 0.64 0.62 0.6  0.8  0.77 0.75 0.73 0.7  0.68 0.66 0.64\n",
            " 0.62 0.6  0.59 0.57 0.55 0.53 0.52 0.5  0.49 0.47 0.46 0.45 0.44 0.7\n",
            " 0.69 0.76 2.38 2.76 2.6  4.29 4.03 3.71 3.45 4.44 4.45 4.46 4.5  4.31\n",
            " 3.95 3.66 3.7  4.26 4.43 4.49 4.49 4.45 4.28 3.93 3.64 4.48 4.46 4.31\n",
            " 4.33 4.4  4.02 4.5  4.34 4.27 3.92 4.44 4.13 4.43 4.28 4.48 4.41 4.42\n",
            " 4.01 4.39 4.18 3.85 4.3  4.34 3.96 3.67 3.81 4.43 4.05 4.13 4.44 4.05\n",
            " 3.73 4.42 4.22 4.27 3.92 4.46 4.07 4.48 4.34 4.45 4.03 3.72 4.29 3.94\n",
            " 4.14 3.81 3.53 3.29 3.08 2.89 2.73 2.59 2.45 2.34 2.22 2.12 2.03 1.95\n",
            " 1.87 1.79 1.72 1.66 1.59 1.54 1.48 1.43 1.38 1.34 1.3  1.25 1.21 1.17\n",
            " 1.14 1.1  1.07 1.04 1.01 0.98 0.95 0.92 0.89 0.87 0.85 0.82 0.8  0.78\n",
            " 0.76 0.74 0.71 0.7  0.68 0.66 0.65 0.63 0.61 0.6  0.58 0.57 0.55 0.54\n",
            " 0.52 0.51 0.49 0.48 0.47 0.46 0.45 0.43 0.43 0.42 0.41 0.39 0.38 0.38\n",
            " 0.37 0.36 0.35 0.34 0.33 0.32 0.31 0.31 0.3  0.29 0.28 0.27 0.27 0.26\n",
            " 0.25 0.25 0.24 0.24 0.23 0.22 0.22 0.22 0.21 0.21 0.2  0.2  0.19 0.18\n",
            " 0.18 0.18 0.17 0.17 0.16 0.16 0.15 0.15 0.15 0.14 0.14 0.14 0.13 0.13\n",
            " 0.12 0.12 0.12 0.11 0.11 0.11 0.11 0.1  0.1  0.1  0.09 0.09 0.09 0.09\n",
            " 0.08 0.08 0.08 0.08 0.08 0.07 0.07 0.07 0.07 0.07 0.06 0.06 0.06 0.06\n",
            " 0.06 0.06 0.06 0.05 0.05 0.05 0.05 0.05 0.05 0.04 0.04 0.04 0.04 0.04\n",
            " 0.04 0.04 0.04 0.03 0.03 0.03 0.04 0.03 0.03 0.03 0.03 0.03 0.03 0.03\n",
            " 0.06 0.05 0.05 0.05 0.05 0.05 0.04 0.04 0.04 0.04 0.07 0.07 0.07 0.08\n",
            " 0.07 0.07 0.07 0.07 0.06 0.06 0.06 0.06 0.06 0.06 0.5  0.47 0.46 0.44\n",
            " 0.43 0.42 0.41 0.39 0.38]\n",
            "label\n",
            "['Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active' 'Active'\n",
            " 'Active' 'Active' 'Active' 'Active' 'Active' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest' 'Rest'\n",
            " 'Rest' 'Rest' 'Rest' 'Rest']\n",
            "train_data\n",
            "               time_stamp  voltage  time   label\n",
            "88   15:38:14.719 -> 3945     3.92  3945  Active\n",
            "46   15:38:14.298 -> 3499     4.43  3499  Active\n",
            "118  15:38:15.035 -> 4263     1.48  4263  Active\n",
            "272  15:38:16.683 -> 5898     0.04  5898    Rest\n",
            "57   15:38:14.401 -> 3615     4.40  3615  Active\n",
            "3    15:38:13.842 -> 3042     0.64  3042    Rest\n",
            "164  15:38:15.531 -> 4752     0.41  4752    Rest\n",
            "139  15:38:15.283 -> 4487     0.78  4487    Rest\n",
            "231  15:38:16.226 -> 5463     0.07  5463    Rest\n",
            "28   15:38:14.087 -> 3307     0.69  3307    Rest\n",
            "133  15:38:15.209 -> 4422     0.92  4422    Rest\n",
            "205  15:38:15.950 -> 5187     0.14  5187    Rest\n",
            "33   15:38:14.155 -> 3360     4.29  3360  Active\n",
            "44   15:38:14.259 -> 3477     3.70  3477  Active\n",
            "183  15:38:15.738 -> 4954     0.25  4954    Rest\n",
            "47   15:38:14.298 -> 3509     4.49  3509  Active\n",
            "66   15:38:14.505 -> 3710     4.28  3710  Active\n",
            "134  15:38:15.209 -> 4433     0.89  4433    Rest\n",
            "173  15:38:15.635 -> 4847     0.32  4847    Rest\n",
            "154  15:38:15.420 -> 4645     0.52  4645    Rest\n",
            "145  15:38:15.317 -> 4550     0.66  4550    Rest\n",
            "120  15:38:15.068 -> 4285     1.38  4285  Active\n",
            "250  15:38:16.436 -> 5664     0.04  5664    Rest\n",
            "213  15:38:16.052 -> 5272     0.11  5272    Rest\n",
            "146  15:38:15.352 -> 4560     0.65  4560    Rest\n",
            "214  15:38:16.052 -> 5282     0.11  5282    Rest\n",
            "43   15:38:14.259 -> 3467     3.66  3467  Active\n",
            "216  15:38:16.087 -> 5304     0.11  5304    Rest\n",
            "73   15:38:14.577 -> 3785     3.85  3785  Active\n",
            "159  15:38:15.493 -> 4699     0.46  4699    Rest\n",
            "..                    ...      ...   ...     ...\n",
            "107  15:38:14.932 -> 4147     2.34  4147  Active\n",
            "194  15:38:15.844 -> 5070     0.19  5070    Rest\n",
            "119  15:38:15.035 -> 4274     1.43  4274  Active\n",
            "93   15:38:14.787 -> 3998     4.45  3998  Active\n",
            "178  15:38:15.669 -> 4900     0.28  4900    Rest\n",
            "223  15:38:16.154 -> 5378     0.09  5378    Rest\n",
            "86   15:38:14.719 -> 3923     4.22  3923  Active\n",
            "0    15:38:13.804 -> 3010     0.68  3010    Rest\n",
            "49   15:38:14.298 -> 3530     4.45  3530  Active\n",
            "261  15:38:16.574 -> 5781     0.03  5781    Rest\n",
            "96   15:38:14.822 -> 4030     4.29  4030  Active\n",
            "103  15:38:14.896 -> 4104     2.89  4104  Active\n",
            "293  15:38:16.892 -> 6121     0.44  6121    Rest\n",
            "14   15:38:13.948 -> 3159     0.62  3159    Rest\n",
            "266  15:38:16.610 -> 5834     0.06  5834    Rest\n",
            "35   15:38:14.155 -> 3382     3.71  3382  Active\n",
            "298  15:38:16.959 -> 6174     0.38  6174    Rest\n",
            "121  15:38:15.068 -> 4295     1.34  4295  Active\n",
            "51   15:38:14.332 -> 3552     3.93  3552  Active\n",
            "45   15:38:14.259 -> 3488     4.26  3488  Active\n",
            "171  15:38:15.599 -> 4826     0.34  4826    Rest\n",
            "70   15:38:14.539 -> 3753     4.01  3753  Active\n",
            "5    15:38:13.842 -> 3063     0.60  3063    Rest\n",
            "158  15:38:15.456 -> 4688     0.47  4688    Rest\n",
            "236  15:38:16.294 -> 5516     0.06  5516    Rest\n",
            "204  15:38:15.950 -> 5176     0.15  5176    Rest\n",
            "239  15:38:16.332 -> 5548     0.06  5548    Rest\n",
            "265  15:38:16.610 -> 5824     0.03  5824    Rest\n",
            "115  15:38:15.002 -> 4232     1.66  4232  Active\n",
            "56   15:38:14.367 -> 3605     4.33  3605  Active\n",
            "\n",
            "[239 rows x 4 columns]\n",
            "test_data\n",
            "               time_stamp  voltage  time   label\n",
            "207  15:38:15.984 -> 5209     0.14  5209    Rest\n",
            "91   15:38:14.752 -> 3977     4.48  3977  Active\n",
            "75   15:38:14.577 -> 3807     4.34  3807  Active\n",
            "248  15:38:16.436 -> 5643     0.04  5643    Rest\n",
            "254  15:38:16.469 -> 5707     0.04  5707    Rest\n",
            "102  15:38:14.860 -> 4093     3.08  4093  Active\n",
            "60   15:38:14.436 -> 3647     4.34  3647  Active\n",
            "174  15:38:15.635 -> 4857     0.31  4857    Rest\n",
            "260  15:38:16.536 -> 5771     0.03  5771    Rest\n",
            "10   15:38:13.910 -> 3117     0.70  3117    Rest\n",
            "277  15:38:16.718 -> 5951     0.07  5951    Rest\n",
            "27   15:38:14.087 -> 3297     0.70  3297    Rest\n",
            "228  15:38:16.226 -> 5431     0.08  5431    Rest\n",
            "244  15:38:16.366 -> 5601     0.05  5601    Rest\n",
            "123  15:38:15.102 -> 4317     1.25  4317  Active\n",
            "98   15:38:14.822 -> 4050     4.14  4050  Active\n",
            "292  15:38:16.892 -> 6110     0.46  6110    Rest\n",
            "83   15:38:14.682 -> 3892     4.05  3892  Active\n",
            "131  15:38:15.172 -> 4402     0.98  4402    Rest\n",
            "36   15:38:14.155 -> 3392     3.45  3392  Active\n",
            "267  15:38:16.610 -> 5844     0.05  5844    Rest\n",
            "282  15:38:16.789 -> 6004     0.07  6004    Rest\n",
            "132  15:38:15.209 -> 4412     0.95  4412    Rest\n",
            "114  15:38:15.002 -> 4220     1.72  4220  Active\n",
            "150  15:38:15.387 -> 4603     0.58  4603    Rest\n",
            "95   15:38:14.787 -> 4019     3.72  4019  Active\n",
            "243  15:38:16.366 -> 5591     0.05  5591    Rest\n",
            "76   15:38:14.611 -> 3817     3.96  3817  Active\n",
            "65   15:38:14.469 -> 3700     4.43  3700  Active\n",
            "69   15:38:14.539 -> 3743     4.42  3743  Active\n",
            "77   15:38:14.611 -> 3828     3.67  3828  Active\n",
            "16   15:38:13.948 -> 3180     0.59  3180    Rest\n",
            "209  15:38:16.018 -> 5229     0.13  5229    Rest\n",
            "230  15:38:16.226 -> 5452     0.07  5452    Rest\n",
            "39   15:38:14.193 -> 3424     4.46  3424  Active\n",
            "195  15:38:15.844 -> 5081     0.18  5081    Rest\n",
            "29   15:38:14.087 -> 3318     0.76  3318    Rest\n",
            "245  15:38:16.401 -> 5611     0.05  5611    Rest\n",
            "74   15:38:14.577 -> 3795     4.30  3795  Active\n",
            "71   15:38:14.539 -> 3764     4.39  3764  Active\n",
            "288  15:38:16.859 -> 6068     0.06  6068    Rest\n",
            "221  15:38:16.120 -> 5357     0.09  5357    Rest\n",
            "274  15:38:16.683 -> 5919     0.04  5919    Rest\n",
            "13   15:38:13.948 -> 3148     0.64  3148    Rest\n",
            "34   15:38:14.155 -> 3371     4.03  3371  Active\n",
            "188  15:38:15.772 -> 5007     0.22  5007    Rest\n",
            "80   15:38:14.644 -> 3860     4.05  3860  Active\n",
            "215  15:38:16.087 -> 5293     0.11  5293    Rest\n",
            "142  15:38:15.283 -> 4518     0.71  4518    Rest\n",
            "87   15:38:14.719 -> 3934     4.27  3934  Active\n",
            "37   15:38:14.193 -> 3403     4.44  3403  Active\n",
            "269  15:38:16.645 -> 5866     0.05  5866    Rest\n",
            "170  15:38:15.599 -> 4815     0.35  4815    Rest\n",
            "162  15:38:15.493 -> 4730     0.43  4730    Rest\n",
            "192  15:38:15.844 -> 5049     0.20  5049    Rest\n",
            "249  15:38:16.436 -> 5654     0.04  5654    Rest\n",
            "259  15:38:16.536 -> 5760     0.03  5760    Rest\n",
            "203  15:38:15.950 -> 5166     0.15  5166    Rest\n",
            "52   15:38:14.332 -> 3562     3.64  3562  Active\n",
            "196  15:38:15.882 -> 5092     0.18  5092    Rest\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2179: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-46-1427e41cad53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mConvNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearing_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m          \u001b[0;31m# set up learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-1427e41cad53>\u001b[0m in \u001b[0;36mConvNet\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m              \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearing_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.001\u001b[0m          \u001b[0;31m# set up learning rate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m   \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m300\u001b[0m                 \u001b[0;31m# split up data in 1 epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeep_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# probablity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'global_step'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'self' is not defined"
          ]
        }
      ]
    }
  ]
}